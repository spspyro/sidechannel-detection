{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hPEI5sBIJh5WpsXR8hsIgLR8WBLFOTrd","timestamp":1702089867563},{"file_id":"1e17EWXYbYUSgRxzMrpw8HganlH4OGQDF","timestamp":1701898134959}],"gpuType":"T4","collapsed_sections":["SdD_gXKHsJ1x"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setting Up"],"metadata":{"id":"oUXD3Bca9E7H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGw7Tsqp7P-t"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import os\n","import torch.utils.data as data\n","import random\n","import shutil\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"KdEcrQnv8hjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1-HC-Pv79aU","executionInfo":{"status":"ok","timestamp":1701884835709,"user_tz":300,"elapsed":12643,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"93c64f2d-0dc3-40ed-dc4c-de5864f1b75e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    print(\"Good to go!\")\n","    DEVICE = torch.device(\"cuda\")\n","else:\n","    print(\"Please set GPU via Edit -> Notebook Settings.\")\n","    DEVICE = torch.device(\"cpu\")\n","\n","\n","# Define some common variables for dtypes/devices.\n","# These can be keyword arguments while defining new tensors.\n","to_float = {\"dtype\": torch.float32, \"device\": DEVICE}\n","to_double = {\"dtype\": torch.float64, \"device\": DEVICE}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ti90qe5H8vQ-","executionInfo":{"status":"ok","timestamp":1701884835878,"user_tz":300,"elapsed":171,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"18d2afef-f816-4f33-fbbc-1e7f866fb3fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Good to go!\n"]}]},{"cell_type":"code","source":["# Put any Preprocessing steps here"],"metadata":{"id":"hK0UVpc-icf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create DataLoader"],"metadata":{"id":"-py3KRKmJRuz"}},{"cell_type":"code","source":["# DATALOADER V2\n","def create_dataset(window, csv_folder):\n","    \"\"\"Transform a time series into a prediction dataset\n","\n","    Args:\n","        dataset: A numpy array of time series, first dimension is the time steps\n","        lookback: Size of window for prediction\n","    \"\"\"\n","    X, y = [], []\n","\n","    data = []\n","    # minlength = 100\n","    droplastrows = 8\n","    rowsperfile = 44 - droplastrows\n","    for file in os.listdir(csv_folder):\n","        if file.endswith('.csv'):\n","            df = pd.read_csv(os.path.join(csv_folder, file))\n","            df.drop(df.tail(1).index,inplace=True)\n","            df.drop(df.head(1).index,inplace=True)\n","            df.replace('<not counted>', np.nan, inplace=True)\n","            df = df.dropna(axis=1, thresh=int(0.5 * len(df)))\n","            df = df.dropna(axis=0, how='any')\n","            df = df.drop(df.tail(8).index)\n","            rows = []\n","            df = df[['L1-icache-load-misses', 'L1-icache-loads']] # FEATURE SELECTION\n","            # df = df[['L1-icache-loads']] # FEATURE SELECTION\n","            # df = df[['L1-icache-load-misses']] # FEATURE SELECTION\n","\n","            for index, row in df.iterrows():\n","              rows.append((row.values).astype(int))\n","            # minlength = min(minlength, len(df))\n","            # self.data.append(rows[:self.rowsperfile])\n","            # print(len(rows[:rowsperfile]), len(rows[0]))\n","            for i in range(rowsperfile - window):\n","                feature = rows[i:i+window]\n","                target = rows[i+window]\n","                # print(len(feature))\n","                # print(len(target))\n","                X.append(feature)\n","                y.append(target)\n","    # print(minlength)\n","\n","    # print(len(X))\n","    # print(len(y))\n","    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"],"metadata":{"id":"PnYynfy86h2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_eval_dataset(window, csv_folder):\n","    \"\"\"Transform a time series into a prediction dataset\n","\n","    Args:\n","        dataset: A numpy array of time series, first dimension is the time steps\n","        lookback: Size of window for prediction\n","    \"\"\"\n","    X, y = [], []\n","    data = []\n","    droplastrows = 8\n","    rowsperfile = 44 - droplastrows\n","    for file in os.listdir(csv_folder):\n","        if file.endswith('.csv'):\n","            df = pd.read_csv(os.path.join(csv_folder, file))\n","            df.drop(df.tail(1).index,inplace=True)\n","            df.drop(df.head(1).index,inplace=True)\n","            df.replace('<not counted>', np.nan, inplace=True)\n","            df = df.dropna(axis=1, thresh=int(0.5 * len(df)))\n","            df = df.dropna(axis=0, how='any')\n","            rows = []\n","            # df = df[['L1-icache-loads']] # FEATURE SELECTION\n","            df = df[['L1-icache-load-misses', 'L1-icache-loads']] # FEATURE SELECTION\n","            # df = df[['L1-icache-load-misses']] # FEATURE SELECTION\n","\n","            for index, row in df.iterrows():\n","              rows.append((row.values).astype(int))\n","            Xf = []\n","            yf = []\n","            for i in range(rowsperfile - window):\n","                feature = rows[i:i+window]\n","                target = rows[i+window]\n","                Xf.append(feature)\n","                yf.append(target)\n","            X.append(Xf)\n","            y.append(yf)\n","    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"],"metadata":{"id":"FApgQfEP28QW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Model"],"metadata":{"id":"lm6WQjn8QVxl"}},{"cell_type":"code","source":["class RNNModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=2, hidden_size=40, num_layers=1, batch_first=True)\n","        self.linear = nn.Linear(40, 2)\n","        self.init_weights(self.linear)\n","        self.init_weights(self.lstm)\n","\n","    def forward(self, x):\n","\n","        batch = x.shape[0]\n","        sequence = x.shape[1]\n","        cols = x.shape[2]\n","\n","        x, _ = self.lstm(x)\n","        x = x[:, -1, :]\n","\n","        x = self.linear(x)\n","        return x\n","\n","\n","    def init_weights(self, layer):\n","        for name, param in layer.named_parameters():\n","            if 'weight' in name:\n","                nn.init.xavier_uniform_(param)\n","            elif 'bias' in name:\n","                nn.init.zeros_(param)"],"metadata":{"id":"kyZUlgI_QYWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/573project/573_LSTM/data2'\n","dir_list = os.listdir(path)\n","\n","print(\"Files and directories in '\", path, \"' :\")\n","\n","# prints all files\n","print(dir_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mE58h9rN_5pP","executionInfo":{"status":"ok","timestamp":1701884838539,"user_tz":300,"elapsed":1436,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"deecb4f5-5c81-4530-f0de-fe4c0c1f0191"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files and directories in ' /content/drive/MyDrive/573project/573_LSTM/data2 ' :\n","['train', 'test', 'train2', 'final']\n"]}]},{"cell_type":"markdown","source":["# Initialize Hyperparamters"],"metadata":{"id":"WupL3jBnQf6g"}},{"cell_type":"code","source":["# Define hyperparameters\n","input_size = 2\n","hidden_size = 64\n","num_layers = 1\n","output_size = 2\n","sequence_length = 20\n","batch_size = 128\n","learning_rate = 1e-4\n","num_epochs = 120\n","data_folder = path\n","train_path = data_folder + '/train2'\n","test_path = data_folder + '/test'\n","\n","# model 3, hidden layer = 40, features: cache misses, learning rate 1e-1, epochs 300, scheduler 0.1 at 200, sequence length 30, don't normalize\n","# model 4, hidden layer = 40, features: cache misses, learning rate 1e-4, epochs 120, sequence length 24, drop the last 8 rows, normalized\n","# model 5, hidden layer = 40, features: cache misses, learning rate 1e-4, epochs 120, sequence length 20, drop the last 8 rows, normalized"],"metadata":{"id":"m9tRW0HxQniH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Randomize some of the training data and split datasets"],"metadata":{"id":"iJGkLuzhgQ3q"}},{"cell_type":"code","source":["# def split_files(source_folder, folder1, folder2, split_ratio=0.88):\n","#     # Get a list of all CSV files in the source folder\n","#     csv_files = [file for file in os.listdir(source_folder) if file.endswith(\".csv\")]\n","\n","#     # Calculate the number of files for each folder based on the split ratio\n","#     num_files_folder1 = int(len(csv_files) * split_ratio)\n","#     num_files_folder2 = len(csv_files) - num_files_folder1\n","\n","#     # Randomly shuffle the list of CSV files\n","#     random.shuffle(csv_files)\n","\n","#     # Split the files into two lists based on the calculated counts\n","#     files_folder1 = csv_files[:num_files_folder1]\n","#     files_folder2 = csv_files[num_files_folder1:]\n","\n","#     # Copy files to folder1\n","#     for file in files_folder1:\n","#         source_path = os.path.join(source_folder, file)\n","#         destination_path = os.path.join(folder1, file)\n","#         shutil.copy2(source_path, destination_path)\n","\n","#     # Copy files to folder2\n","#     for file in files_folder2:\n","#         source_path = os.path.join(source_folder, file)\n","#         destination_path = os.path.join(folder2, file)\n","#         shutil.copy2(source_path, destination_path)\n","\n","#     print(f\"Number of files in {folder1}: {len(files_folder1)}\")\n","#     print(f\"Number of files in {folder2}: {len(files_folder2)}\")\n","\n","# # Example usage\n","# source_folder = data_folder + '/train'\n","# folder1 = data_folder + '/train2'\n","# folder2 = data_folder + '/final'\n","\n","# split_files(source_folder, folder1, folder2, split_ratio=0.88)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOK2d2VBfe0_","executionInfo":{"status":"ok","timestamp":1701721418284,"user_tz":300,"elapsed":6191,"user":{"displayName":"Shivan Prasad","userId":"09561767476633507041"}},"outputId":"6f97893d-5a20-4e85-ebd7-5d3a8adbae39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of files in /content/drive/MyDrive/573project/573_LSTM/data2/train2: 565\n","Number of files in /content/drive/MyDrive/573project/573_LSTM/data2/final: 78\n"]}]},{"cell_type":"markdown","source":["#Initialize Loader"],"metadata":{"id":"KdByIvMp-Gf-"}},{"cell_type":"code","source":["# Create DataLoader V2\n","X_train, y_train = create_dataset(sequence_length, train_path)\n","X_test, y_test = create_dataset(sequence_length, test_path)\n","loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size, drop_last=True)\n","\n","print(X_train.shape)\n"],"metadata":{"id":"up9fCTnX685x","executionInfo":{"status":"ok","timestamp":1701884855909,"user_tz":300,"elapsed":11136,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2d3d6ca-c355-431a-f7e1-6f18bf59fa05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bc77fe16ec0c>:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([9040, 20, 2])\n"]}]},{"cell_type":"code","source":["# # Normalize DataLoader - ONLY RUN ONCE, comment out max1 values if only using 1 feature, comment out both if not normalizing\n","\n","Xtrain_max0 = torch.max(X_train[:,:,0]) # maximum column value between each data split\n","Xtrain_max1 = torch.max(X_train[:,:,1])\n","\n","ytrain_max0 = torch.max(y_train[:,0]) # maximum column value between each data split\n","ytrain_max1 = torch.max(y_train[:,1])\n","\n","\n","Xtest_max0 = torch.max(X_test[:,:,0])\n","Xtest_max1 = torch.max(X_test[:,:,1])\n","\n","ytest_max0 = torch.max(y_test[:,0])\n","ytest_max1 = torch.max(y_test[:,1])\n","\n","max0 = max(Xtrain_max0, Xtest_max0, ytrain_max0, ytest_max0)\n","max1 = max(Xtrain_max1, Xtest_max1, ytrain_max1, ytest_max1)\n","\n","\n","X_train[:,:,0] /= max0\n","X_train[:,:,1] /= max1\n","\n","X_test[:,:,0] /= max0\n","X_test[:,:,1] /= max1\n","\n","y_train[:,0] /= max0\n","y_train[:,1] /= max1\n","\n","y_test[:,0] /= max0\n","y_test[:,1] /= max1\n"],"metadata":{"id":"5Fdwq0yp2yhp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Initialize Model and Start Training"],"metadata":{"id":"qnZtF5gD-ONS"}},{"cell_type":"code","source":["# Create RNN model\n","rnn = RNNModel()"],"metadata":{"id":"qAcE32vjAtqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=200, gamma=0.1)\n","\n","# # Set patience and other hyperparameters\n","# patience = 20\n","# best_test_rmse = float('inf')\n","# counter = 0\n","\n","for epoch in range(num_epochs):\n","    rnn.train()\n","    for X_batch, y_batch in loader:\n","        y_pred = rnn(X_batch)\n","        loss = criterion(y_pred, y_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    scheduler.step()\n","    rnn.eval()\n","    with torch.no_grad():\n","        y_pred_train = rnn(X_train)\n","        train_rmse = np.sqrt(criterion(y_pred_train, y_train))\n","        y_pred_test = rnn(X_test)\n","        test_rmse = np.sqrt(criterion(y_pred_test, y_test))\n","\n","    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n","\n","    # # Check for early stopping\n","    # if test_rmse < best_test_rmse:\n","    #     best_test_rmse = test_rmse\n","    #     counter = 0\n","    #     torch.save(rnn.state_dict(), 'rnn_model.pth')\n","    # else:\n","    #     counter += 1\n","\n","    # if counter >= patience:\n","    #     print(f\"Early stopping after {epoch} epochs.\")\n","    #     break\n","\n","\n","# Save the trained model\n","# torch.save(rnn.state_dict(), 'rnn_model.pth')"],"metadata":{"id":"2u7O03O8Au7E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701885003176,"user_tz":300,"elapsed":123701,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"566536a8-7427-4017-dddc-c2071b2867e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train RMSE 0.2387, test RMSE 0.2440\n","Epoch 1: train RMSE 0.1905, test RMSE 0.1935\n","Epoch 2: train RMSE 0.1471, test RMSE 0.1470\n","Epoch 3: train RMSE 0.1198, test RMSE 0.1192\n","Epoch 4: train RMSE 0.0974, test RMSE 0.0977\n","Epoch 5: train RMSE 0.0880, test RMSE 0.0888\n","Epoch 6: train RMSE 0.0850, test RMSE 0.0860\n","Epoch 7: train RMSE 0.0834, test RMSE 0.0845\n","Epoch 8: train RMSE 0.0820, test RMSE 0.0834\n","Epoch 9: train RMSE 0.0810, test RMSE 0.0827\n","Epoch 10: train RMSE 0.0802, test RMSE 0.0819\n","Epoch 11: train RMSE 0.0797, test RMSE 0.0815\n","Epoch 12: train RMSE 0.0791, test RMSE 0.0810\n","Epoch 13: train RMSE 0.0789, test RMSE 0.0808\n","Epoch 14: train RMSE 0.0784, test RMSE 0.0804\n","Epoch 15: train RMSE 0.0783, test RMSE 0.0803\n","Epoch 16: train RMSE 0.0780, test RMSE 0.0800\n","Epoch 17: train RMSE 0.0778, test RMSE 0.0799\n","Epoch 18: train RMSE 0.0776, test RMSE 0.0798\n","Epoch 19: train RMSE 0.0775, test RMSE 0.0797\n","Epoch 20: train RMSE 0.0774, test RMSE 0.0795\n","Epoch 21: train RMSE 0.0772, test RMSE 0.0795\n","Epoch 22: train RMSE 0.0771, test RMSE 0.0794\n","Epoch 23: train RMSE 0.0770, test RMSE 0.0793\n","Epoch 24: train RMSE 0.0769, test RMSE 0.0792\n","Epoch 25: train RMSE 0.0768, test RMSE 0.0792\n","Epoch 26: train RMSE 0.0768, test RMSE 0.0791\n","Epoch 27: train RMSE 0.0766, test RMSE 0.0789\n","Epoch 28: train RMSE 0.0765, test RMSE 0.0788\n","Epoch 29: train RMSE 0.0765, test RMSE 0.0789\n","Epoch 30: train RMSE 0.0765, test RMSE 0.0787\n","Epoch 31: train RMSE 0.0762, test RMSE 0.0786\n","Epoch 32: train RMSE 0.0761, test RMSE 0.0785\n","Epoch 33: train RMSE 0.0760, test RMSE 0.0784\n","Epoch 34: train RMSE 0.0759, test RMSE 0.0783\n","Epoch 35: train RMSE 0.0758, test RMSE 0.0782\n","Epoch 36: train RMSE 0.0758, test RMSE 0.0781\n","Epoch 37: train RMSE 0.0758, test RMSE 0.0781\n","Epoch 38: train RMSE 0.0756, test RMSE 0.0780\n","Epoch 39: train RMSE 0.0756, test RMSE 0.0781\n","Epoch 40: train RMSE 0.0756, test RMSE 0.0782\n","Epoch 41: train RMSE 0.0753, test RMSE 0.0778\n","Epoch 42: train RMSE 0.0753, test RMSE 0.0778\n","Epoch 43: train RMSE 0.0754, test RMSE 0.0778\n","Epoch 44: train RMSE 0.0752, test RMSE 0.0777\n","Epoch 45: train RMSE 0.0751, test RMSE 0.0775\n","Epoch 46: train RMSE 0.0750, test RMSE 0.0775\n","Epoch 47: train RMSE 0.0750, test RMSE 0.0775\n","Epoch 48: train RMSE 0.0749, test RMSE 0.0774\n","Epoch 49: train RMSE 0.0749, test RMSE 0.0774\n","Epoch 50: train RMSE 0.0748, test RMSE 0.0772\n","Epoch 51: train RMSE 0.0747, test RMSE 0.0771\n","Epoch 52: train RMSE 0.0746, test RMSE 0.0771\n","Epoch 53: train RMSE 0.0747, test RMSE 0.0771\n","Epoch 54: train RMSE 0.0746, test RMSE 0.0771\n","Epoch 55: train RMSE 0.0745, test RMSE 0.0770\n","Epoch 56: train RMSE 0.0744, test RMSE 0.0768\n","Epoch 57: train RMSE 0.0743, test RMSE 0.0768\n","Epoch 58: train RMSE 0.0743, test RMSE 0.0768\n","Epoch 59: train RMSE 0.0743, test RMSE 0.0767\n","Epoch 60: train RMSE 0.0742, test RMSE 0.0766\n","Epoch 61: train RMSE 0.0741, test RMSE 0.0766\n","Epoch 62: train RMSE 0.0742, test RMSE 0.0766\n","Epoch 63: train RMSE 0.0741, test RMSE 0.0766\n","Epoch 64: train RMSE 0.0739, test RMSE 0.0764\n","Epoch 65: train RMSE 0.0740, test RMSE 0.0764\n","Epoch 66: train RMSE 0.0739, test RMSE 0.0764\n","Epoch 67: train RMSE 0.0738, test RMSE 0.0762\n","Epoch 68: train RMSE 0.0737, test RMSE 0.0762\n","Epoch 69: train RMSE 0.0737, test RMSE 0.0761\n","Epoch 70: train RMSE 0.0737, test RMSE 0.0761\n","Epoch 71: train RMSE 0.0736, test RMSE 0.0761\n","Epoch 72: train RMSE 0.0735, test RMSE 0.0760\n","Epoch 73: train RMSE 0.0737, test RMSE 0.0761\n","Epoch 74: train RMSE 0.0734, test RMSE 0.0758\n","Epoch 75: train RMSE 0.0734, test RMSE 0.0758\n","Epoch 76: train RMSE 0.0736, test RMSE 0.0759\n","Epoch 77: train RMSE 0.0734, test RMSE 0.0759\n","Epoch 78: train RMSE 0.0733, test RMSE 0.0757\n","Epoch 79: train RMSE 0.0735, test RMSE 0.0758\n","Epoch 80: train RMSE 0.0733, test RMSE 0.0756\n","Epoch 81: train RMSE 0.0731, test RMSE 0.0755\n","Epoch 82: train RMSE 0.0732, test RMSE 0.0756\n","Epoch 83: train RMSE 0.0731, test RMSE 0.0754\n","Epoch 84: train RMSE 0.0732, test RMSE 0.0755\n","Epoch 85: train RMSE 0.0730, test RMSE 0.0753\n","Epoch 86: train RMSE 0.0730, test RMSE 0.0754\n","Epoch 87: train RMSE 0.0729, test RMSE 0.0753\n","Epoch 88: train RMSE 0.0729, test RMSE 0.0752\n","Epoch 89: train RMSE 0.0729, test RMSE 0.0753\n","Epoch 90: train RMSE 0.0727, test RMSE 0.0751\n","Epoch 91: train RMSE 0.0729, test RMSE 0.0753\n","Epoch 92: train RMSE 0.0728, test RMSE 0.0751\n","Epoch 93: train RMSE 0.0730, test RMSE 0.0754\n","Epoch 94: train RMSE 0.0726, test RMSE 0.0749\n","Epoch 95: train RMSE 0.0727, test RMSE 0.0751\n","Epoch 96: train RMSE 0.0727, test RMSE 0.0750\n","Epoch 97: train RMSE 0.0725, test RMSE 0.0749\n","Epoch 98: train RMSE 0.0726, test RMSE 0.0747\n","Epoch 99: train RMSE 0.0725, test RMSE 0.0747\n","Epoch 100: train RMSE 0.0731, test RMSE 0.0750\n","Epoch 101: train RMSE 0.0725, test RMSE 0.0746\n","Epoch 102: train RMSE 0.0724, test RMSE 0.0746\n","Epoch 103: train RMSE 0.0725, test RMSE 0.0746\n","Epoch 104: train RMSE 0.0724, test RMSE 0.0745\n","Epoch 105: train RMSE 0.0722, test RMSE 0.0744\n","Epoch 106: train RMSE 0.0726, test RMSE 0.0750\n","Epoch 107: train RMSE 0.0723, test RMSE 0.0744\n","Epoch 108: train RMSE 0.0721, test RMSE 0.0743\n","Epoch 109: train RMSE 0.0723, test RMSE 0.0743\n","Epoch 110: train RMSE 0.0724, test RMSE 0.0748\n","Epoch 111: train RMSE 0.0721, test RMSE 0.0741\n","Epoch 112: train RMSE 0.0720, test RMSE 0.0742\n","Epoch 113: train RMSE 0.0719, test RMSE 0.0741\n","Epoch 114: train RMSE 0.0720, test RMSE 0.0740\n","Epoch 115: train RMSE 0.0721, test RMSE 0.0740\n","Epoch 116: train RMSE 0.0722, test RMSE 0.0741\n","Epoch 117: train RMSE 0.0722, test RMSE 0.0741\n","Epoch 118: train RMSE 0.0719, test RMSE 0.0738\n","Epoch 119: train RMSE 0.0717, test RMSE 0.0738\n"]}]},{"cell_type":"code","source":["# Print the parameters of the linear layer\n","for name, param in rnn.named_parameters():\n","    if 'linear' in name:\n","        print(name, param.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6AeiQ63tV_g","executionInfo":{"status":"ok","timestamp":1701885003176,"user_tz":300,"elapsed":3,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"da2b528c-2636-49de-e50f-e49e56dcffdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["linear.weight tensor([[-0.1732,  0.2164,  0.1395,  0.3702,  0.1945,  0.2789, -0.0166, -0.1718,\n","         -0.0074,  0.2269,  0.2882, -0.1131, -0.2905,  0.1939, -0.2095,  0.1330,\n","          0.0924, -0.1697, -0.0667, -0.2685, -0.3346,  0.4074,  0.1989, -0.3295,\n","          0.2016, -0.0260,  0.0234, -0.2846, -0.2959, -0.2918,  0.1549, -0.3485,\n","         -0.0458,  0.2945,  0.1110, -0.3384,  0.4037, -0.4134, -0.2427,  0.1755],\n","        [-0.0536, -0.1497,  0.3887,  0.0632,  0.0492,  0.2987,  0.3459, -0.0797,\n","         -0.0744, -0.0258,  0.2252,  0.2337,  0.3164,  0.0460, -0.2775, -0.3190,\n","         -0.1317, -0.0972,  0.0981,  0.2519,  0.3461,  0.1436, -0.3761,  0.1724,\n","         -0.0320, -0.2653,  0.1357,  0.2534,  0.1481, -0.2829,  0.3363,  0.0213,\n","          0.3066, -0.0620, -0.1722, -0.1617,  0.0200,  0.0488, -0.2316,  0.4142]])\n","linear.bias tensor([-0.0021,  0.0027])\n"]}]},{"cell_type":"markdown","source":["#Anomaly Detection\n"],"metadata":{"id":"r78z_UgRZj7c"}},{"cell_type":"markdown","source":["Inference: Error at timestep t = mean squared error between prediction and ground truth. To detect anomaly, use a D-length decision window to determine if for all timesteps in this window, the error is too high (greater than a specified threshold).\n","\n","Small decision window length (D) = shorter detection time and less leakage, but higher false positive rate.\n","\n","Test below.\n","\n","Need to utilize separate dataset for both vulnerable and non-vulnerable files.\n","\n","How?\n","\n","  - Loop through rows in CSV file, and begin detection at certain timestep. Then monitor decision window afterwards."],"metadata":{"id":"xTisbp9VZpjV"}},{"cell_type":"code","source":["dir_list = os.listdir(path + '/final/vuln')\n","\n","print(\"Files and directories in '\", path + '/final/vuln', \"' :\")\n","\n","# prints all files\n","print(len(dir_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLQA_P29dov6","executionInfo":{"status":"ok","timestamp":1701885009618,"user_tz":300,"elapsed":6443,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"df8f37e0-483d-43d6-a950-7ef0dd2c4555"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files and directories in ' /content/drive/MyDrive/573project/573_LSTM/data2/final/vuln ' :\n","799\n"]}]},{"cell_type":"code","source":["# Creating evaluation datasets, comment out normalization step if training set was not normalized\n","X_vuln_eval, y_vuln_eval = create_eval_dataset(sequence_length, path + '/final/vuln')\n","X_nonvuln_eval, y_nonvuln_eval = create_eval_dataset(sequence_length, path + '/final/nonvuln')\n","\n","X_nonvuln_eval[:,:,:,0] /= max0\n","X_nonvuln_eval[:,:,:,1] /= max1\n","\n","X_vuln_eval[:,:,:,0] /= max0\n","X_vuln_eval[:,:,:,1] /= max1\n","\n","y_nonvuln_eval[:,:,0] /= max0\n","y_nonvuln_eval[:,:,1] /= max1\n","\n","y_vuln_eval[:,:,0] /= max0\n","y_vuln_eval[:,:,1] /= max1"],"metadata":{"id":"pKNv31LRrXNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","# print(X_test.shape)\n","print(X_nonvuln_eval.shape)\n","print(y_nonvuln_eval.shape)\n","\n","# print(X_nonvuln_eval[1,2,:,:])\n","# print(y_nonvuln_eval[1,1,:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9MxtGH7whK7","executionInfo":{"status":"ok","timestamp":1701885044105,"user_tz":300,"elapsed":9,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"a86b3b5a-0f93-4940-94fe-21842384c459"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([9040, 20, 2])\n","torch.Size([78, 16, 20, 2])\n","torch.Size([78, 16, 2])\n"]}]},{"cell_type":"code","source":["# print((X_nonvuln_eval[1,1,:,:]))\n","print(X_batch.shape)\n","# print(X_nonvuln_eval[0,0,:,:].view(1, -1, 1).shape)\n","print(X_nonvuln_eval[20,0,:,:])\n","print(rnn(X_nonvuln_eval[20,0,:,:].view(1, -1, 1)))\n","print(y_nonvuln_eval[20,0,:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3bVAEAJ09II","executionInfo":{"status":"ok","timestamp":1701885044229,"user_tz":300,"elapsed":132,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"b8384694-093b-4af6-9bc6-70f633bf82da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128, 20, 2])\n","tensor([[0.5163, 0.2184],\n","        [0.4750, 0.2324],\n","        [0.5252, 0.1512],\n","        [0.4969, 0.1423],\n","        [0.4640, 0.3380],\n","        [0.4839, 0.1405],\n","        [0.5075, 0.1396],\n","        [0.4980, 0.1423],\n","        [0.5136, 0.1399],\n","        [0.4078, 0.5212],\n","        [0.5473, 0.1702],\n","        [0.4649, 0.1334],\n","        [0.5296, 0.1453],\n","        [0.5319, 0.1478],\n","        [0.5296, 0.1490],\n","        [0.5305, 0.1477],\n","        [0.5338, 0.1455],\n","        [0.5281, 0.1482],\n","        [0.4668, 0.1320],\n","        [0.2776, 0.8815]])\n","tensor([[-0.1037, -0.2674]], grad_fn=<AddmmBackward0>)\n","tensor([0.5511, 0.1692])\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Assuming you have two tensors 'tensor1' and 'tensor2' with the same shape\n","tensor1 = rnn(X_nonvuln_eval[1,1,:,:].view(1, -1, 2))\n","tensor2 = y_nonvuln_eval[1,1,:].view(1,2)\n","\n","# Calculate MSE\n","mse = F.mse_loss(tensor1, tensor2)\n","\n","print(mse)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dQLIk0m5y2O","executionInfo":{"status":"ok","timestamp":1701885044229,"user_tz":300,"elapsed":2,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"9feb51d8-7f97-41b4-ee08-bae97eaf8795"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.7726e-06, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"code","source":["# Anomaly Detection Implementation 1\n","tn, tp, fn, fp = 0,0,0,0\n","threshold = 6e-7\n","percent = 1\n","# for model 1, use 9e-4 to obtain: True positive:  570 , True negative:  22 , False positive:  56 , False negative:  229\n","# for model 2, use 2e-4 to obtain: True positive:  544 , True negative:  22 , False positive:  56 , False negative:  255\n","# for model 3, use 2e2 to obtain: True positive:  487 , True negative:  31 , False positive:  47 , False negative:  312\n","# normalized model 3, use 9e-6 to obtain: True positive:  525 , True negative:  25 , False positive:  53 , False negative:  274\n","# normalized model 4, use 7e-7 to obtain: True positive:  536 , True negative:  25 , False positive:  53 , False negative:  263\n","# normalized model 5, use 1e-7 to obtain: True positive:  550 , True negative:  27 , False positive:  51 , False negative:  249 or True positive:  470 , True negative:  36 , False positive:  42 , False negative:  329\n","# multivariate model: use 2e-7 to obtain: True positive:  601 , True negative:  16 , False positive:  62 , False negative:  198\n","print(\"Non-vulnerable application detection\")\n","avgerror = 0\n","for i in range(X_nonvuln_eval.shape[0]):\n","  flags = 0\n","\n","  for j in range(X_nonvuln_eval.shape[1]):\n","    error = F.mse_loss(rnn(X_nonvuln_eval[i,j,:,:].view(1, X_nonvuln_eval.shape[2], X_nonvuln_eval.shape[3])), y_nonvuln_eval[i,j,:].view(1,X_nonvuln_eval.shape[3]))\n","    # print(error, \" and \" ,threshold)\n","    # print(error)\n","    avgerror += error\n","    if error >  threshold:\n","      flags += 1\n","  if flags >= (percent * X_nonvuln_eval.shape[1]):\n","    fp += 1\n","  else:\n","    tn += 1\n","print(avgerror / (X_nonvuln_eval.shape[0] * X_nonvuln_eval.shape[1]))\n","print(\"Vulnerable application detection\")\n","avgerror = 0\n","for i in range(X_vuln_eval.shape[0]):\n","  flags = 0\n","  for j in range(X_vuln_eval.shape[1]):\n","    error = F.mse_loss(rnn(X_vuln_eval[i,j,:,:].view(1, X_vuln_eval.shape[2], X_nonvuln_eval.shape[3])), y_vuln_eval[i,j,:].view(1,X_nonvuln_eval.shape[3]))\n","    avgerror += error\n","    if error > threshold:\n","      flags += 1\n","  if flags >= (percent * X_vuln_eval.shape[1]):\n","    tp += 1\n","  else:\n","    fn += 1\n","    # print(flags)\n","print(avgerror / (X_vuln_eval.shape[0] * X_vuln_eval.shape[1]))\n","print(\"True positive: \", tp, \", True negative: \", tn, \", False positive: \", fp , \", False negative: \", fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fefSpRWwWLIl","executionInfo":{"status":"ok","timestamp":1701885624266,"user_tz":300,"elapsed":7190,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"a7370b44-441e-4792-b13c-31155939a4a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Non-vulnerable application detection\n","tensor(0.0050, grad_fn=<DivBackward0>)\n","Vulnerable application detection\n","tensor(0.0063, grad_fn=<DivBackward0>)\n","True positive:  550 , True negative:  26 , False positive:  52 , False negative:  249\n"]}]},{"cell_type":"markdown","source":["# Testing Transformer Model"],"metadata":{"id":"ka7QkKmYRTkF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        num_layers = 1\n","        input_size = 2\n","        # Replace LSTM with Transformer layer\n","        self.transformer = nn.Transformer(\n","            d_model=input_size,\n","            nhead=1,  # Number of heads in the multiheadattention models\n","            num_encoder_layers=num_layers,\n","            num_decoder_layers=num_layers,\n","        )\n","        self.linear = nn.Linear(2, 2)\n","\n","    def forward(self, x):\n","\n","        x = x.permute(1, 0, 2)\n","        x = self.transformer(x, x)\n","        x = x.permute(1, 0, 2)\n","        x = x[:, -1, :]\n","\n","        x = self.linear(x)\n","        return x\n"],"metadata":{"id":"1A_KXBpURVOS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer = TransformerModel()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OySDmvu9R-dc","executionInfo":{"status":"ok","timestamp":1701860846181,"user_tz":300,"elapsed":293,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"5fe1a508-71ef-4d18-d318-22484fd38765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["learning_rate = 0.001\n","num_epochs = 5\n","\n","# Loss and optimizer\n","criterion2 = nn.MSELoss()\n","optimizer2 = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_epochs):\n","    transformer.train()\n","    for X_batch, y_batch in loader:\n","        y_pred = transformer(X_batch)\n","        loss = criterion2(y_pred, y_batch)\n","        optimizer2.zero_grad()\n","        loss.backward()\n","        optimizer2.step()\n","    # scheduler.step()\n","    transformer.eval()\n","    with torch.no_grad():\n","        y_pred = transformer(X_train)\n","        train_rmse = np.sqrt(criterion2(y_pred, y_train))\n","        y_pred = transformer(X_test)\n","        test_rmse = np.sqrt(criterion2(y_pred, y_test))\n","    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n","\n","\n","\n","\n","\n","# Save the trained model\n","# torch.save(rnn.state_dict(), 'rnn_model.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwVNQyKfR7pH","executionInfo":{"status":"ok","timestamp":1701861034799,"user_tz":300,"elapsed":73750,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"09722d92-ef1d-463d-fbe3-250344c975dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train RMSE 0.1101, test RMSE 0.1138\n","Epoch 1: train RMSE 0.1102, test RMSE 0.1138\n","Epoch 2: train RMSE 0.1101, test RMSE 0.1138\n","Epoch 3: train RMSE 0.1102, test RMSE 0.1138\n","Epoch 4: train RMSE 0.1101, test RMSE 0.1139\n"]}]},{"cell_type":"markdown","source":["#Transformer Anomaly Detection"],"metadata":{"id":"NO1GHrU90CVZ"}},{"cell_type":"code","source":["# Anomaly Detection Implementation 1\n","tn, tp, fn, fp = 0,0,0,0\n","threshold = 4.5e-4\n","percent = 1\n","# with threshold 4e-4, obtain: True positive:  645 , True negative:  17 , False positive:  61 , False negative:  154\n","# with threshold 5e-4, obtain: True positive:  479 , True negative:  36 , False positive:  42 , False negative:  320\n","# with threshold 4.5e-4, obtain: True positive:  593 , True negative:  24 , False positive:  54 , False negative:  206\n","print(\"Non-vulnerable application detection\")\n","avgerror = 0\n","for i in range(X_nonvuln_eval.shape[0]):\n","  flags = 0\n","  for j in range(X_nonvuln_eval.shape[1]):\n","    error = F.mse_loss(transformer(X_nonvuln_eval[i,j,:,:].view(1, X_nonvuln_eval.shape[2], X_nonvuln_eval.shape[3])), y_nonvuln_eval[i,j,:].view(1,X_nonvuln_eval.shape[3]))\n","    # print(error, \" and \" ,threshold)\n","    # print(error)\n","    avgerror += error\n","    if error >  threshold:\n","      flags += 1\n","  if flags >= (percent * X_nonvuln_eval.shape[1]):\n","    fp += 1\n","  else:\n","    tn += 1\n","print(avgerror / (X_nonvuln_eval.shape[0] * X_nonvuln_eval.shape[1]))\n","print(\"Vulnerable application detection\")\n","avgerror = 0\n","for i in range(X_vuln_eval.shape[0]):\n","  flags = 0\n","  for j in range(X_vuln_eval.shape[1]):\n","    error = F.mse_loss(transformer(X_vuln_eval[i,j,:,:].view(1, X_vuln_eval.shape[2], X_nonvuln_eval.shape[3])), y_vuln_eval[i,j,:].view(1,X_nonvuln_eval.shape[3]))\n","    avgerror += error\n","    if error > threshold:\n","      flags += 1\n","  if flags >= (percent * X_vuln_eval.shape[1]):\n","    tp += 1\n","  else:\n","    fn += 1\n","    # print(flags)\n","print(avgerror / (X_vuln_eval.shape[0] * X_vuln_eval.shape[1]))\n","print(\"True positive: \", tp, \", True negative: \", tn, \", False positive: \", fp , \", False negative: \", fn)"],"metadata":{"id":"30HTQyUV0FZP","executionInfo":{"status":"ok","timestamp":1701861742360,"user_tz":300,"elapsed":34378,"user":{"displayName":"Epic Gamer_YT","userId":"04361489543036344725"}},"outputId":"8dd30b84-5681-4c73-af50-08b37d11c159","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Non-vulnerable application detection\n","tensor(0.0138, grad_fn=<DivBackward0>)\n","Vulnerable application detection\n","tensor(0.0139, grad_fn=<DivBackward0>)\n","True positive:  593 , True negative:  24 , False positive:  54 , False negative:  206\n"]}]},{"cell_type":"markdown","source":["# Try https://unit8co.github.io/darts/generated_api/darts.models.forecasting.dlinear.html and TimeGPT (in-progress)"],"metadata":{"id":"SdD_gXKHsJ1x"}},{"cell_type":"code","source":["from nixtlats import TimeGPT"],"metadata":{"id":"B6NzRKSYyBAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["timegpt = TimeGPT(token='vXQjOsslTogkzysd8CCD972XqgJhhpH21xeZ2sqnCGTNAk37F5iRC7fYXUDkZQahoOlv3NQb2IH6Lm07y18jjcFLqyDeuzs2wOMkckC9b6wV5X6r5w3ErT76TRvuO6LUxBpZGDVDHWNCPXdbkR6eTQ2Q4vqn7h1EaGgY436SQGe8PdILP0Yy1BUrhOc9cCGZZCz5PJG4IzzxOXov5dzSmOONijnKPifmeinxXI6UAj4JdJVjDU1sARLYpC6Sa2NJ')"],"metadata":{"id":"fQMHH44Aahe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if timegpt.validate_token():\n","    print(\"Token validation successful!\")  # Token is valid.\n","else:\n","    raise Exception(\"Token validation failed! Please check go to https://dashboard.nixtla.io/ to get your token.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFidjClOa7Q6","executionInfo":{"status":"ok","timestamp":1701837536088,"user_tz":300,"elapsed":1160,"user":{"displayName":"Shivan Prasad","userId":"09561767476633507041"}},"outputId":"c13fd462-67b7-43d8-ab53-6f0b6e68a866"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token validation successful!\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/573project/573_LSTM/data2'\n","dir_list = os.listdir(path + '/final/nonvuln')\n","\n","print(\"Files and directories in '\", path, \"' :\")\n","\n","# prints all files\n","print(dir_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE2pX8NLb7EG","executionInfo":{"status":"ok","timestamp":1701837891957,"user_tz":300,"elapsed":83,"user":{"displayName":"Shivan Prasad","userId":"09561767476633507041"}},"outputId":"7ccc05f7-d12c-4db4-9b5d-6e2573a7984b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files and directories in ' /content/drive/MyDrive/573project/573_LSTM/data2 ' :\n","['f_rand104.txt.10.stat (3).csv', 'f_rand133.txt.10.stat (2).csv', 'f_rand118.txt.10.stat (2).csv', 'f_rand135.txt.10.stat (3).csv', 'f_rand171.txt.10.stat (2).csv', 'f_rand193.txt.10.stat (2).csv', 'f_rand14.txt.10.stat (2).csv', 'f_rand149.txt.10.stat (3).csv', 'f_rand20.txt.10.stat (2).csv', 'f_rand27.txt.10.stat (3).csv', 'f_rand5.txt.10.stat (3).csv', 'f_rand52.txt.10.stat (3).csv', 'f_rand96.txt.10.stat (3).csv', 'f_rand68.txt.10.stat (2).csv', 'f_rand90.txt.10.stat (3).csv', 'f_rand9.txt.10.stat.csv', 'f_rand97.txt.10.stat (3).csv', 'f_rand81.txt.10.stat (3).csv', 'f_rand78.txt.10.stat (3).csv', 'f_rand93.txt.10.stat (3).csv', 'f_10_rand101.txt.10.stat.csv', 'f_10_rand158.txt.10.stat.csv', 'f_10_rand150.txt.10.stat.csv', 'f_10_rand126.txt.10.stat.csv', 'f_10_rand20.txt.10.stat.csv', 'f_10_rand166.txt.10.stat.csv', 'f_10_rand183.txt.10.stat.csv', 'f_10_rand198.txt.10.stat.csv', 'f_10_rand18.txt.10.stat.csv', 'f_10_rand21.txt.10.stat.csv', 'f_10_rand161.txt.10.stat.csv', 'f_10_rand171.txt.10.stat.csv', 'f_10_rand24.txt.10.stat.csv', 'f_10_rand67.txt.10.stat.csv', 'f_10_rand53.txt.10.stat.csv', 'f_10_rand26.txt.10.stat.csv', 'f_10_rand68.txt.10.stat.csv', 'f_10_rand4.txt.10.stat.csv', 'f_10_rand25.txt.10.stat.csv', 'f_10_rand83.txt.10.stat.csv', 'f_10_rand91.txt.10.stat.csv', 'f_10_rand79.txt.10.stat.csv', 'f_10_rand78.txt.10.stat.csv', 'f_rand105.txt.10.stat (2).csv', 'f_rand152.txt.10.stat (2).csv', 'f_rand163.txt.10.stat (2).csv', 'f_rand184.txt.10.stat.csv', 'f_rand167.txt.10.stat.csv', 'f_rand177.txt.10.stat (2).csv', 'f_rand175.txt.10.stat (2).csv', 'f_rand196.txt.10.stat (2).csv', 'f_rand19.txt.10.stat (2).csv', 'f_rand28.txt.10.stat.csv', 'f_rand6.txt.10.stat (2).csv', 'f_rand37.txt.10.stat (2).csv', 'f_rand44.txt.10.stat (2).csv', 'f_rand42.txt.10.stat.csv', 'f_rand48.txt.10.stat (2).csv', 'f_rand65.txt.10.stat.csv', 'f_rand64.txt.10.stat (2).csv', 'f_rand78.txt.10.stat (2).csv', 'f_rand98.txt.10.stat (2).csv', 'f_rand137.txt.10.stat.csv', 'f_rand121.txt.10.stat.csv', 'f_rand105.txt.10.stat.csv', 'f_rand128.txt.10.stat.csv', 'f_rand132.txt.10.stat.csv', 'f_rand110.txt.10.stat.csv', 'f_rand188.txt.10.stat.csv', 'f_rand185.txt.10.stat.csv', 'f_rand31.txt.10.stat.csv', 'f_rand192.txt.10.stat.csv', 'f_rand191.txt.10.stat.csv', 'f_rand48.txt.10.stat.csv', 'f_rand199.txt.10.stat.csv', 'f_rand73.txt.10.stat.csv', 'f_rand83.txt.10.stat.csv', 'f_rand90.txt.10.stat.csv']\n"]}]}]}